{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: Yusuf Elnady\n",
    "## Project: Chatbot - RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import unicodedata\n",
    "import codecs\n",
    "import random\n",
    "import torch\n",
    "import itertools\n",
    "from torch import nn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading From Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# filename=\"/projects/3e080ba6-1476-4b72-953b-1b591cbf600a/Notebooks/Yusuf/cornell movie-dialogs corpus/movie_lines.txt\"\n",
    "filename=\"movie_lines.txt\"\n",
    "\n",
    "with open(filename, 'r', encoding='iso-8859-1') as datafile:\n",
    "     lines = datafile.readlines()\n",
    "\n",
    "all_text_and_fields = {}\n",
    "\n",
    "for line in lines:\n",
    "    temp_dict = {}\n",
    "    temp_list = line.split(' +++$+++ ')\n",
    "    all_text_and_fields[temp_list[0]] ={'lineID':temp_list[0], 'characterID':temp_list[1],\n",
    "                                       'movieID':temp_list[2], 'characterName':temp_list[3],\n",
    "                                       'text':temp_list[4]}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# filename2=\"/projects/3e080ba6-1476-4b72-953b-1b591cbf600a/Notebooks/Yusuf/cornell movie-dialogs corpus/movie_conversations.txt\"\n",
    "filename2=\"movie_conversations.txt\"\n",
    "with open(filename2, 'r', encoding='iso-8859-1') as datafile:\n",
    "     lines = datafile.readlines()\n",
    "        \n",
    "        \n",
    "conversations = []\n",
    "pattern = re.compile('L[0-9]+')\n",
    "for line in lines:\n",
    "    temp_list = line.split(' +++$+++ ')\n",
    "    temp_dict = {'Character1ID':temp_list[0], 'Character2ID':temp_list[1],\n",
    "                  'movieID':temp_list[2], 'lineIDs':temp_list[3]}\n",
    "    \n",
    "    my_line_ids = pattern.findall(temp_dict['lineIDs'])\n",
    "    my_lines_text = []\n",
    "    for line_id in my_line_ids:\n",
    "        my_lines_text.append(all_text_and_fields[line_id]['text'])\n",
    "    \n",
    "    temp_dict['lines'] = my_lines_text\n",
    "    conversations.append(temp_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Character1ID': 'u0',\n",
       " 'Character2ID': 'u2',\n",
       " 'movieID': 'm0',\n",
       " 'lineIDs': \"['L194', 'L195', 'L196', 'L197']\\n\",\n",
       " 'lines': ['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\n',\n",
       "  \"Well, I thought we'd start with pronunciation, if that's okay with you.\\n\",\n",
       "  'Not the hacking and gagging and spitting part.  Please.\\n',\n",
       "  \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\"]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building QA Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def make_qa_pairs(conversations):\n",
    "\n",
    "    qa_pairs = []\n",
    "\n",
    "    for conversation in conversations:\n",
    "        for i in range(0,len(conversation['lines'])-1,2):\n",
    "            q = conversation['lines'][i].strip()\n",
    "            a = conversation['lines'][i+1].strip()\n",
    "\n",
    "            if q and a:\n",
    "                qa_pairs.append([q,a])\n",
    "                \n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "qa_pairs = make_qa_pairs(conversations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving clear Version of qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "datafile = 'qa_pairs.txt'\n",
    "delimiter = '\\t'\n",
    "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
    "\n",
    "with open(datafile , 'w',encoding='utf-8') as outputfile:\n",
    "    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n",
    "    for pair in qa_pairs:\n",
    "        writer.writerow(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "datafile = 'qa_pairs.txt'\n",
    "with open(datafile,\"r\") as outputfile:  # Had to add encoding to split the list\n",
    "    lines=outputfile.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing and Adding The Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0 \n",
    "SOS_TOKEN = 1\n",
    "EOS_TOKEN = 2\n",
    "class Voc:\n",
    "    def __init__(self,name):\n",
    "        self.name = name\n",
    "        self.word2index = {'PAD':0,'SOS':1,'EOS':2 }\n",
    "        self.index2word = {0:'PAD', 1:'SOS', 2:'EOS'}\n",
    "        self.num_words = 3\n",
    "        self.word2count = {'PAD':1,'SOS':1,'EOS':1}\n",
    "    def add_sentence(self,sentence): # sentence is just a long string, and I will split it into many strings\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word.strip())\n",
    "        \n",
    "    def add_word(self,word):\n",
    "        if word in self.word2index:\n",
    "            self.word2count[word]+=1\n",
    "        elif word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicode2ascii(s): # Turn a Unicode string to plain ASCII\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalize_word(word):\n",
    "    \n",
    "    contractions_dict = { \"ain't\": \"are not\", \"'s\":\" is\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\", \"I'm\": \"I am\", \"I've\": \"I have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"they'd\": \"they would\", \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
    " \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what've\": \"what have\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where've\": \"where have\",\n",
    " \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who've\": \"who have\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "    \n",
    "    word=unicode2ascii(word.lower().strip())\n",
    "    \n",
    "    for key in contractions_dict.keys():\n",
    "        if key in word:\n",
    "            word=word.replace(key,contractions_dict[key])\n",
    "    \n",
    "    word=re.sub(r\"([.!,?'])\",r\" \\1 \",word)\n",
    "    word=re.sub(r\"[^a-zA-Z.!,?']+\",r\" \",word)\n",
    "    word=re.sub(r\"\\s+\",r\" \",word).strip()\n",
    "    \n",
    "    return word\n",
    "\n",
    "def normalize_sentence(sentence):\n",
    "    normalized_sentence = ''\n",
    "    for word in sentence.split(' '):\n",
    "        normalized_sentence += normalize_word(word)+' '\n",
    "    return normalized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 10  # Maximum sentence length to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "voc = Voc('My Vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#This function takes the qa_pairs, and normalize everyword, and then adds the words to the vocabulary class\n",
    "def build_vocabulary(voc, qa_pairs):\n",
    "    keep_pairs = [ ]\n",
    "    qa_pairs = [[normalize_sentence(sentence) for sentence in pair] for pair in qa_pairs]\n",
    "    for pair in qa_pairs:\n",
    "        #roxaane wasn't in the vocabulary, because the sentence length was larger than MAX_SENTENCE_LENGTH\n",
    "        if (len(pair[0].split(' '))<=MAX_SENTENCE_LENGTH and len(pair[1].split(' '))<=MAX_SENTENCE_LENGTH):\n",
    "            voc.add_sentence(pair[0])\n",
    "            voc.add_sentence(pair[1])\n",
    "            keep_pairs.append(pair)\n",
    "    return voc, keep_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "voc, normalized_qa_pairs = build_vocabulary(voc, qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Values without deleting anything', 14199, 14199)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Values without deleting anything\" ,len(voc.index2word.keys()), voc.num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trim Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem I had --> When we trim words, we should change the index of the other words to be in the range of the num_vocabulary\n",
    "\n",
    "# Which means if Vocab is 20, then trimmed to be 10, so no word should have index [11-20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_words(voc):\n",
    "    words_kept = []\n",
    "    trimmed_words = []\n",
    "    for word,count in voc.word2count.items():\n",
    "        if count >= WORD_FREQ_THRESHOLD:\n",
    "            words_kept.append(word)\n",
    "        elif count<WORD_FREQ_THRESHOLD:\n",
    "            trimmed_words.append(word)\n",
    "                \n",
    "     # Reinitialize dictionaries\n",
    "    voc.word2index = {}\n",
    "    voc.word2count = {}\n",
    "    voc.index2word = {PAD_TOKEN: \"PAD\", SOS_TOKEN: \"SOS\", EOS_TOKEN: \"EOS\"}\n",
    "    voc.num_words = 3 # Count default tokens\n",
    "\n",
    "    for word in words_kept:\n",
    "        voc.add_word(word)           \n",
    "    return voc, trimmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "voc, trimmed_words = trim_words(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def remove_trimmed_pairs(qa_pairs,trimmed_words):\n",
    "    keep_pairs = []\n",
    "    for pair in qa_pairs:\n",
    "        flag_q = True\n",
    "        flag_a = True\n",
    "        \n",
    "        question = pair[0]\n",
    "        answer = pair[1]\n",
    "        \n",
    "        for word in question.split(' '):\n",
    "            if word in trimmed_words:\n",
    "                flag_q = False \n",
    "                break \n",
    "    \n",
    "        if flag_q == True:\n",
    "            for word in answer.split(' '):\n",
    "                if word in trimmed_words:\n",
    "                    flag_a = False \n",
    "                    break \n",
    "    \n",
    "        if flag_q and flag_a :\n",
    "            keep_pairs.append(pair)\n",
    "            \n",
    "    return keep_pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "final_qa_pairs = remove_trimmed_pairs(normalized_qa_pairs,trimmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['there . ', 'where ? '],\n",
       " ['you have my word .  as a gentleman ', 'you are sweet . '],\n",
       " ['hi . ', 'looks like things worked out tonight , huh ? '],\n",
       " ['have fun tonight ? ', 'tons '],\n",
       " ['well , no . . . ', 'then that is all you had to say . '],\n",
       " ['but ', 'you always been this selfish ? '],\n",
       " ['do you listen to this crap ? ', 'what crap ? '],\n",
       " ['wow ', 'let is go . '],\n",
       " ['she okay ? ', 'i hope so . '],\n",
       " ['they do to ! ', 'they do not ! '],\n",
       " ['did you change your hair ? ', 'no . '],\n",
       " ['it is more ', 'expensive ? '],\n",
       " ['where have you been ? ', 'nowhere . . . hi , daddy . '],\n",
       " ['in th .  for a month ', 'why ? '],\n",
       " ['he was , like , a total babe ', 'but you hate joey '],\n",
       " ['you looked beautiful last night , you know . ', 'so did you '],\n",
       " ['let go ! ', 'you set me up . '],\n",
       " ['but she does not want to date . ', 'exactly my point '],\n",
       " ['daddy , i  ', \"and where ' re you going ? \"],\n",
       " ['oh , god .  it is starting . ', 'it is just a party . daddy . '],\n",
       " ['you the new guy ? ', 'so they tell me . . . '],\n",
       " ['how many people were in your old school ? ', 'thirty two . '],\n",
       " ['get out ! ', 'how many people go here ? '],\n",
       " ['what about him ? ', 'you wanna go out with him ? '],\n",
       " ['her favorite uncle ', 'dead at forty one . '],\n",
       " ['he is pretty ! ', 'okay !  i was not sure '],\n",
       " [\"cameron , i ' m a little busy \", 'it is off . the whole thing . '],\n",
       " ['cameron  do you like the girl ? ', 'sure '],\n",
       " ['what is the worst ? ', 'you get the girl . '],\n",
       " ['hey  do you mind ? ', 'not at all '],\n",
       " ['where ya goin ? ', 'away . '],\n",
       " ['leave my sister alone . ', 'and why would i do that ? '],\n",
       " ['yeah ', 'what do you think ? '],\n",
       " ['a hundred bucks a date . ', 'forget it . '],\n",
       " ['it is about time . ', 'a deal is a deal . '],\n",
       " ['how did you do it ? ', 'do what ? '],\n",
       " ['hey . ', 'are you lost ? '],\n",
       " ['he always look so ', 'block e ? '],\n",
       " [\"you think this ' ll work ? \", 'no fear . '],\n",
       " [\"what ' d he say ? \", 'who cares ? '],\n",
       " ['have you seen him ? ', 'who ? '],\n",
       " ['pick you up friday , then ', 'oh , right .  friday . '],\n",
       " ['i say , do what you wanna do . ', 'funny , you are the only one '],\n",
       " ['okay ? ', \"i ' m fine . i ' m \"],\n",
       " ['you are not okay . ', 'i just need to lie down for awhile '],\n",
       " [\"why ' re you doing this ? \", 'i told you '],\n",
       " ['you do not care if i die ', 'sure , i do '],\n",
       " ['i thought you were above all that ', 'you know what they say '],\n",
       " ['kat ! wake up ! ', 'what ? '],\n",
       " ['were you in jail ? ', 'maybe . '],\n",
       " ['no , you were not ', \"then why ' d you ask ? \"],\n",
       " ['i should do this . ', 'do what ? '],\n",
       " ['start a band ? ', 'my father would not approve of that that '],\n",
       " ['i heard there was a poetry reading . ', \"you ' re so  \"],\n",
       " ['so what is your excuse ? ', 'acting the way we do . '],\n",
       " ['then you screwed up ', 'how ? '],\n",
       " ['you up for it ? ', 'for . . . ? '],\n",
       " ['what ? ', 'no one else knows '],\n",
       " ['is that a request or a command ? ', 'you know what i mean '],\n",
       " ['no . ', 'no what ? '],\n",
       " ['no , i will not go with you ', 'why not ? '],\n",
       " ['my grandmother is . ', 'what ? '],\n",
       " ['why cannot we agree on this ? ',\n",
       "  'because you are making decisions for me . '],\n",
       " ['hey there .  tired of breathing ? ', 'hi . '],\n",
       " ['cool pictures .  you a fan ? ', 'yeah .  i guess . '],\n",
       " ['you think ? ', 'oh yeah . '],\n",
       " ['kat a fan , too ? ', 'yeah . . . '],\n",
       " ['say it ', 'what ? '],\n",
       " ['what ? ! ', 'good enough . '],\n",
       " ['ever been to club skunk ? ', 'yeah . '],\n",
       " ['have a great time , honey ! ', 'but  who  what ? '],\n",
       " ['what just happened ? ', 'your daughters went to the prom . '],\n",
       " ['did i have anything to say about it ? ', 'absolutely not . '],\n",
       " ['he is the devil is child . . . ', 'we will all go crazy . . . '],\n",
       " ['i could be gone for years . ', 'i know . '],\n",
       " ['she said yes . ', 'thank god . . . '],\n",
       " ['cannot you stay with us a little ? ', 'i am busy inside . '],\n",
       " ['my letters of appointment . ', 'appointment to what ? '],\n",
       " ['i want to go with you ! ', \"there ' ll be a time . \"],\n",
       " ['no . . . ', 'no ? '],\n",
       " ['you come !  you speak first ! ', 'tell the chief we thank him . '],\n",
       " ['to bring the word of god . ', 'chief says  he has a god . '],\n",
       " ['. . . and also to bring medicine . ', 'chief says . . . '],\n",
       " ['all of them !  just lies ! ', 'colon !  do not ! '],\n",
       " ['we cannot raise the wheel without it . ', 'my horse does not work . '],\n",
       " ['what do you read ? ', 'twenty eight . '],\n",
       " ['where can i meet this man ? ', 'immediately . '],\n",
       " ['do not say anything . ', 'where are we going ? '],\n",
       " [\"i ' m coming with you . \", 'yes .  yes , come with me ! '],\n",
       " ['are you married ? ', 'divorced . '],\n",
       " ['i cannot take you to my place . ', 'somewhere else ? '],\n",
       " ['you considered becoming a prostitute ? ', 'yes , i considered it . '],\n",
       " ['did you ever turn tricks before ? ', 'no . '],\n",
       " ['what about back home ? ', 'no . '],\n",
       " ['are you alright ? ', 'i still cannot believe eddie is gone . '],\n",
       " ['you better get packed . ', 'right . '],\n",
       " ['do you have coffee ? ', 'in the kitchen . '],\n",
       " [\"i ' ll make some for us . \", \"i ' ll get my clothes . \"],\n",
       " ['oh . ', 'it was my decision , not his . '],\n",
       " ['who did cause and origin ? ', 'who do you think , chief ? ! '],\n",
       " ['chief  mind if i take her ? ', 'okay .  but not water sports . '],\n",
       " ['coffee for me , i gotta slow down . ', 'vodka tonic . '],\n",
       " ['tomorrow .  at lunch . ', 'you ready ? '],\n",
       " ['where is she ? ', \"takin ' a bath . \"],\n",
       " ['only one guys checked in ? ', 'yeah . '],\n",
       " ['are you hit ? ', \"no .  i ' m okay . \"],\n",
       " ['no . ', 'did you tell him you did ? '],\n",
       " ['you know what that is , right ? ', 'no , what is it ? '],\n",
       " ['my folks are . ', 'stay here . '],\n",
       " ['mind if i ride along with you ? ',\n",
       "  'this has nothing to do with your fire . '],\n",
       " ['two heads . ', 'better than one . '],\n",
       " ['you thirsty ? ', \"i ' m on duty . \"],\n",
       " ['why not ? ', 'something back home ? '],\n",
       " ['yeah . ', 'what would your girlfriend think of that ? '],\n",
       " ['i do not have a girlfriend . ', 'my point exactly . '],\n",
       " [\"i ' m serious here . \", 'so am i . '],\n",
       " ['so . . . who is nicky ? ', 'what do you want ? '],\n",
       " ['i kill someone famous . ', 'then do it , asshole . '],\n",
       " ['how many victims are up there ? ',\n",
       "  'there are two bodies found at this point . '],\n",
       " ['alright . ', 'alright ? '],\n",
       " ['alright . ', 'alright . '],\n",
       " ['hey , honey . ', 'hey . '],\n",
       " ['yeah . ', 'well , that is great . '],\n",
       " ['okay , til tonight . ', 'tonight . '],\n",
       " ['you promise ? ', 'yeah .  i promise . '],\n",
       " ['okay . ', 'see you later .  good luck . '],\n",
       " ['thank you . ', 'do not be late . '],\n",
       " ['ready ? ', 'keep them out of my way . '],\n",
       " ['okay .  you ready ? ', 'yeah , yeah .  jesus . '],\n",
       " ['and what do you do with the bones ? ', 'dog food . '],\n",
       " [\"hi , i ' m honey . \", 'where is czech girl ? '],\n",
       " ['how much money are you carrying with you ? ',\n",
       "  'i have five hundred dollars . '],\n",
       " ['please join us .  come on forward . ', 'is there a problem ? '],\n",
       " ['do not fool around . ', 'okay . '],\n",
       " ['did you hear what i said ? ', 'i want to document my trip to america . '],\n",
       " ['speak english ! ', 'you said speak czech ! '],\n",
       " ['let me get a shot of it . ', 'sit down ! '],\n",
       " [\"i ' m serious . \", 'shut up .  look ! '],\n",
       " ['you got that ? ', 'no , i do not get that ! '],\n",
       " ['i can get you a job . ', 'a job ? '],\n",
       " ['yes , the money is good . ', 'as a plumber ? ! '],\n",
       " ['robert . . . ? ', 'what are you doing here ? '],\n",
       " ['well , he . . . ', 'i want to hear everything he said . '],\n",
       " [\"i ' m trying to tell you . \", 'alright .  go ahead . '],\n",
       " ['a cheap hotel . ', 'what are you coming here to do ? '],\n",
       " ['no .  go ahead . ', 'thanks .  appreciate it . '],\n",
       " ['what was that ? ', 'evidence .  of a homicide . '],\n",
       " [\"i ' ll take him . \", 'no way !  he is mine ! '],\n",
       " ['mouth is clean , too . ', 'clean ? '],\n",
       " ['how was it ? ', 'not good . '],\n",
       " ['get outta here ! ', 'what the hell happened ? '],\n",
       " ['continued ', 'she is wonderful . '],\n",
       " ['okay . ', 'okay , come on down . '],\n",
       " ['really . ', 'yes . '],\n",
       " ['i know . it would be almost inconceivable . ',\n",
       "  'but not completely inconceivable ? '],\n",
       " ['you are kidding . ', 'no . '],\n",
       " ['hal ? ', 'yes . '],\n",
       " ['well , do not worry about it . ', 'and do not you worry about it . '],\n",
       " ['too bad about frank , is not it ? ', 'yes , it is . '],\n",
       " ['continued ', 'did they have any explanation for this ? '],\n",
       " ['continued ', 'well , what is it ? '],\n",
       " [\"i ' m having a party tomorrow . \", 'yes , i know that sweetheart . '],\n",
       " ['when are you coming home ? ', 'in three days , darling , i hope . '],\n",
       " ['where is mrs . brown ? ', 'she is in the bathroom . '],\n",
       " ['oh , thank you very much . ', 'thank you . '],\n",
       " ['that is right . ', 'i see . '],\n",
       " ['that is true . ', 'thank you very much , hal . '],\n",
       " ['yeah ? ', 'i want to pick up my car . '],\n",
       " ['name ? ', 'hammond . '],\n",
       " ['this is three years old . ', \"yeah , i ' ve been busy . \"],\n",
       " ['yeah . ', 'vodka . '],\n",
       " ['i want to drive awhile . ', 'i are not tired yet . '],\n",
       " ['pay money ? ', 'yeah , dummy .  money . '],\n",
       " ['you are in a hurry . ', 'yeah , i been waiting three years . '],\n",
       " ['i hear you have got visitors . ', 'would you guys . . . '],\n",
       " ['you are gonna help us take him . ', 'no chance . '],\n",
       " ['luther was part of the gang ? ', \"what gang you talkin ' about , jack ? \"],\n",
       " ['hey , this works pretty good . ', 'thank you . '],\n",
       " ['i do not give out the details . ',\n",
       "  'last night , two nights ago , three ? '],\n",
       " ['last night . ', 'you have a good time ? '],\n",
       " ['there . ', 'must be billy is girl . '],\n",
       " ['half a million . ', 'jesus . '],\n",
       " ['why ? ', 'luther is on the move . . . '],\n",
       " ['notice something funny about that bus ? ',\n",
       "  'yeah . it missed the last four stops . '],\n",
       " ['hey , there she is . . . ', 'whatever play i maker just back me up . '],\n",
       " ['what the bell happened ? ', 'i lost them , that is what happened . '],\n",
       " ['tell me that is not the same guy . ', 'hey . . . dick tracy . '],\n",
       " ['hey , you are right . ', 'you are hopeless . '],\n",
       " [\"i ' m all wet . \", 'what is wrong with that ? '],\n",
       " ['call me later . ', 'you sure you want me to ? '],\n",
       " ['great place for lunch . ', 'yeah , one of my favorites . '],\n",
       " ['hello . ', 'hi , it is me . . . '],\n",
       " ['hey , i do not believe it . ', 'hiya , kid . '],\n",
       " ['three more hours . . . ', 'where is he ? '],\n",
       " ['you are impossible . . . ', 'that is what i always say . '],\n",
       " ['hard man to live with . ', 'how would you know ? '],\n",
       " ['hey , two days with him is enough . ', 'that is no bull . '],\n",
       " ['hey . . . ', 'shut up . '],\n",
       " ['stall . ', 'what do you want ? '],\n",
       " [\"how ya doin ' ? \", 'cannot complain . '],\n",
       " ['we got a lot to talk about . ', 'yeah , old times . '],\n",
       " ['when ? ', 'i cannot get it until monday . honest . '],\n",
       " ['let her go . ', 'first , the money . '],\n",
       " ['how you doing , man ? ', 'not bad , not bad . '],\n",
       " ['you want to go outside ? ', 'naw , right here is okay . '],\n",
       " ['five .  on credit . ', 'this are not a credit business . '],\n",
       " ['. . . we are saved ! ', \"i ' m fucked ! \"],\n",
       " ['. . . would you like a drink ? ', 'no thank you . '],\n",
       " ['who are you ? ', 'i brought the girl remember ? '],\n",
       " ['he is a she ! ', 'you noticed . . . '],\n",
       " ['we have to save the world . ', 'good luck . . '],\n",
       " ['leeloo is in trouble ? ', 'when is she not in trouble ? '],\n",
       " ['is there anything that can stop it ? ', 'yes . . thank god . . '],\n",
       " ['it is a miracle ! ! ! ', 'what is ? '],\n",
       " ['they really made her . . . ', 'perfect . '],\n",
       " ['yeah ? ', 'hey bud ! finger here . '],\n",
       " ['i just found a picture of you . ', 'how do i look ? '],\n",
       " ['uh . . . at least fifty . ', 'in your dreams ! see you tonight ! '],\n",
       " ['yeah ? ', 'have you pulled yourself together ? '],\n",
       " [\"i ' m sorry . . \", 'this is a police control action . . '],\n",
       " ['. . . . . . ', 'found it ? '],\n",
       " ['. . . hi . ', 'does it get any better or what ! '],\n",
       " ['you know how to fly this thing ? ', 'it is like a cab is not it ? '],\n",
       " ['to save the world . ', 'where have i heard this song before ? '],\n",
       " ['do not you open your messages ? ',\n",
       "  \"i ' ve had enough good news for today \"],\n",
       " [\"i ' m not going . \", 'why not ? '],\n",
       " ['. . . shit ! ', 'what is it ? '],\n",
       " ['it is my wife . ', 'i thought you were divorced . '],\n",
       " ['you hear that ? ', 'cornelius . . '],\n",
       " ['finished what ? ', 'learning language . '],\n",
       " ['which one ? ', 'all . '],\n",
       " ['you noticed . . ', 'ok , you can turn around ! '],\n",
       " ['it is up to you now , angel ! ', \"i ' m so tired . . . \"],\n",
       " ['. . . like love . . . ', 'exactly . '],\n",
       " ['tell me . . . ', 'i love you . . . '],\n",
       " ['the cash man ! ', 'been here long ? '],\n",
       " ['thanks . . ', 'you are welcome . . '],\n",
       " ['this is all that survived ? ', 'actually only one cell survived . . '],\n",
       " ['is that better ? ', 'perfect , mr . president . '],\n",
       " ['welcome home . ', 'do you know how much i missed you ? '],\n",
       " ['i love you . ', 'i love you . '],\n",
       " ['you could not ? ', 'you do not understand . . . '],\n",
       " ['you are killing me . . . ', 'do not . . . '],\n",
       " ['who are you calling ? ', 'mrs . christian . '],\n",
       " ['anything harder ? ', 'there is nothing harder . '],\n",
       " ['bullshit . ', \"i ' m telling you . . . \"],\n",
       " ['what the fuck . . . ! ', 'i promised him to machine . '],\n",
       " ['that is five thousand dollars . ', 'is it ? '],\n",
       " ['i want to watch you work . ', \"i ' ll consider it . \"],\n",
       " ['what is the problem ? ', \"i ' m camera shy . \"],\n",
       " ['where is that ? ', 'brooklyn .  do not be late . '],\n",
       " ['you brought the money ? ', 'right here . '],\n",
       " ['excellent . ', 'where are the women ? '],\n",
       " ['you are a dead man . ', 'leave him alone . '],\n",
       " ['i figured you share information . ', 'we do . '],\n",
       " ['yeah , who is this ? ', 'i know what you did . '],\n",
       " ['what ? ', 'i know what you did . '],\n",
       " [\"i ' m gonna kill you . \", 'do not bore me with that bullshit . '],\n",
       " ['do not ask questions . ', 'fuck you ! '],\n",
       " ['starting to recognize a pattern ? ', 'what do you want ? '],\n",
       " ['who is machine ? ', 'i do not know . . . '],\n",
       " ['i want his name . ', 'i told you , i do not know . '],\n",
       " ['how much did he pay you ? ',\n",
       "  'thirty thousand each , that fucking cocksucker . '],\n",
       " ['do it . ', 'do not think i will not . '],\n",
       " ['you are a private investigator ? ', 'that is right . '],\n",
       " ['you are asking me why ? ', \"i ' m asking . \"],\n",
       " ['you almost went over your limit . ', 'fuck you . '],\n",
       " ['give me the film . ', 'you will get it when we get there . '],\n",
       " ['big date tonight ? ', 'yeah . . . guess so . '],\n",
       " ['how long you been working there ? ', 'three , four years . '],\n",
       " ['how much do you make now ? ', 'four hundred a week , off the books . '],\n",
       " ['how old are you ? ', 'twenty five . '],\n",
       " ['where are your parents ? ', 'i do not know , where are yours ? '],\n",
       " ['you are learning . ', 'where does he sell it ? '],\n",
       " ['hello . . . ? ', \"i ' m here . \"],\n",
       " ['hello ? ', 'mrs . christian , tom welles here . '],\n",
       " ['what was she running from ? ', 'i do not know . '],\n",
       " ['why do you think he did it ? ', 'it got to be too much for him . '],\n",
       " ['alice , you think you can leave ? ', 'what is wrong ? '],\n",
       " ['kincaid and joey died last night . ', 'what ? '],\n",
       " [\"i ' m sure they are around . \", \"yeah , i ' m not so sure . \"],\n",
       " ['you have been up all night ? ', 'that obvious , huh ? '],\n",
       " ['no . . . ', 'you are his sister , right ? '],\n",
       " [\"i ' ve been working double shifts . \", 'extra money , huh ? '],\n",
       " ['how long have you been awake ? ', 'three days . '],\n",
       " ['not really . ', 'is there something we can do ? '],\n",
       " ['you look great ! ', 'save it for later .  come on ! '],\n",
       " ['t t thanks alice . . . ', 'earth to alice . . . '],\n",
       " ['i do not get it . ', 'let me talk to you . '],\n",
       " ['something wrong with the stairs ? ', 'avoid all contact day . '],\n",
       " ['what is it ? ', 'oh god !  he killed them ! '],\n",
       " ['excuse us , dear . ', 'it is okay , dan '],\n",
       " ['hey . . . wake up . ', 'huh ? '],\n",
       " [\"i ' m sorry your boyfriend got killed . \", 'how did you know that ? '],\n",
       " ['is that who you are waiting for ? ', 'no . . . '],\n",
       " ['who says i do not like you ? ', 'my friend , with the funny hand . '],\n",
       " ['hi , beautiful . ', 'jesus !  do not do that ! '],\n",
       " ['there you go .  love you . ', 'me too . '],\n",
       " ['where are we going ? ', 'to take a picture . '],\n",
       " ['very funny . ', 'alice . . . '],\n",
       " ['alice ! ', \"i ' ve got to go . \"],\n",
       " ['you do good work , alice . ', 'so did dan . '],\n",
       " ['mark , are you okay ? ', \"yeah .  i ' m just aces . \"],\n",
       " ['then get out ! ', 'mark ! '],\n",
       " ['who is jacob ? ', 'my baby ! '],\n",
       " [\"they think i ' m nuts . \", 'that is their problem . '],\n",
       " ['poor woman . . . ', 'no shit . '],\n",
       " ['stop saying that , it is bullshit . ', 'i want to talk about the baby . '],\n",
       " ['it is okay . ', 'stick around , please ? '],\n",
       " ['you , too ? ', 'he invited me to his house last night . '],\n",
       " ['yeah ? ', 'listen !  hear that ? '],\n",
       " ['you hear that ? ', 'the sound again ? '],\n",
       " ['well ? ', \"i ' m with u you ! u \"],\n",
       " ['is there any way out of it ? ', 'seems to be all around . . . '],\n",
       " ['what about down ? ', 'i . . . do not know ! '],\n",
       " ['what course , skipper ? ', 'right at our one eyed friend ! '],\n",
       " ['as we near the pole . . . ', 'there is got to be an explanation ! '],\n",
       " ['what is the running time ? ', 'thirty four seconds ! '],\n",
       " ['right u at u him ? ', 'that is what i said ! '],\n",
       " ['he did ! ', 'i am afraid not .  therefore  '],\n",
       " [\"well , i ' ll be  ! \", \"i ' ll be another ! \"],\n",
       " ['what are you boys up to ? ', 'same old shit . '],\n",
       " ['i plan to . soon . ', 'how soon ? '],\n",
       " ['you heard the news . ', 'i hear twombley got shot . '],\n",
       " ['he is on to us ! ', 'shit ! what are we gonna do ? '],\n",
       " ['we bought you . ', 'that was me . '],\n",
       " ['if you get it ? ', 'yeah . '],\n",
       " ['you might not kill it . ', 'you think so . '],\n",
       " ['i used to play ball . ', 'yeah ? '],\n",
       " ['double a . new britain . ', 'oh . '],\n",
       " ['they said . ', 'hmm . '],\n",
       " ['safety on ? ', 'yeah . '],\n",
       " ['this way . ', 'sun is gettin high . '],\n",
       " ['bastard is got his high beams on . ', 'shit . '],\n",
       " ['where did twombley get shot ? ', 'in the chest . '],\n",
       " ['you stayed away ? ', 'yeah . '],\n",
       " ['where did you get the blood ? ', 'what blood ? '],\n",
       " ['sure you did . ', 'what ? '],\n",
       " [\"i ' m fucking out of here . \", 'lawford ? '],\n",
       " ['why do they do that ? ', 'do what ? '],\n",
       " ['you know . ', 'break stuff ? '],\n",
       " ['yeah . it is stupid . ', 'i guess they are stupid . '],\n",
       " ['was it funny ? ', 'to us it was . '],\n",
       " ['i bet you did lots of bad things . ', 'what are you talking about ? '],\n",
       " ['what then ? ', 'it is stupid . '],\n",
       " ['what ? you called mommy ? just now ? ', 'yes . '],\n",
       " ['it is pretty old . ', 'it belongs to pop . '],\n",
       " ['pop ? ', 'grandpa . my father . it is his . '],\n",
       " ['what do you want , then ? ', 'nothing . '],\n",
       " ['please do not cry . please , honey . ', 'what are you sorry for ? '],\n",
       " ['that is illegal , you know . ', 'i know . '],\n",
       " ['you do not want the extra police pay ? ', \"i ' m not saying that . \"],\n",
       " ['what is the hurry ? ', 'a hunting accident . jack and twombley . '],\n",
       " ['huh ? ', 'i figured you already heard . '],\n",
       " ['how you holding up , wade ? ', \"i ' m fine , fine . \"],\n",
       " ['i saw mel gordon in here this morning . ', 'so ? '],\n",
       " ['i will not . you interested ? ', 'maybe . '],\n",
       " ['you and mel gordon ? ', 'could be . '],\n",
       " ['lillian ! ', 'where is jill ? '],\n",
       " ['we should talk . ', 'we have done all our talking , wade . '],\n",
       " ['see . get your boots . ', 'hi honey . '],\n",
       " ['did you get his number ? ', 'i know who it is . '],\n",
       " ['good . who ? ', 'mel gordon . '],\n",
       " ['how is she doing ? ', 'okay . she is fine . '],\n",
       " ['you okay ? ', 'yeah . '],\n",
       " ['you okay ? ', 'yeah . '],\n",
       " [\"i ' m sorry about what i said . \", 'said what ? '],\n",
       " ['you do not mean that . ', 'yeah . i mean that . '],\n",
       " ['call me . ', 'tonight . let is get together . '],\n",
       " ['money . ', 'jack does not need money . '],\n",
       " [\"alright . i ' ll think about it . \", 'good . '],\n",
       " ['strange . ', 'think they are alright ? '],\n",
       " ['of course ! i would have heard . ', 'how ? '],\n",
       " ['this is nuts . ', 'wade . '],\n",
       " ['it makes me sad . ', 'can ? '],\n",
       " ['it do not look right . ', 'what ? '],\n",
       " ['just do it . ', 'atta go . '],\n",
       " ['where is ma ? ', 'she is coming . '],\n",
       " ['is there something wrong with the phone ? ', 'in the living room . '],\n",
       " ['call it what you want . ', 'everything you know is from me . '],\n",
       " ['yeah . ', 'bang ! '],\n",
       " ['who ? ', 'twombley . '],\n",
       " ['no shit . ', 'you think jack shot him ? '],\n",
       " ['lillian was here . in lawford . ', 'huh ? '],\n",
       " ['the night before the shooting . ', 'how was she ? '],\n",
       " ['what about margie ? ', 'what about her ? '],\n",
       " ['which is ? ', 'that it was not an accident . '],\n",
       " ['motive . you gotta have a motive . ', 'money . '],\n",
       " ['mom ? ', 'yes dear ? '],\n",
       " ['alice ! ', 'daddy . . . '],\n",
       " ['daddy . . . ', 'alice . . . i . . . '],\n",
       " ['how you doing , sweetie ? ', 'been better , dad . . . you ? '],\n",
       " ['there they are ! ', \"okay , i ' m slowing us down . \"],\n",
       " ['do you see the maintenance panel ? ', 'got it . '],\n",
       " ['put your pilot on . ', 'he is busy being dead . '],\n",
       " ['who is flying the fucking plane ? ', \"i ' m doing what i can . \"],\n",
       " ['got it . ', 'stay cool . '],\n",
       " ['the chief justice ? what on earth for ? ',\n",
       "  'to swear you in as president . '],\n",
       " ['why ? ', 'because it is my duty . '],\n",
       " ['the rest of the secret service ? ', 'dead . '],\n",
       " ['how many others killed ? ', 'nine . '],\n",
       " ['now , or he dies , please . ', 'come on , alice . '],\n",
       " ['what do you know of my husband ? ', 'i know he left you behind . '],\n",
       " ['do you have to be so brutal ? ', 'yes '],\n",
       " ['four . . . ', 'jim . . . '],\n",
       " ['stop . ', 'you will do it ? '],\n",
       " [\"i ' ll not going without my family . \", 'yes , sir . '],\n",
       " ['sir , we stay with the president . ', 'that is not necessary . '],\n",
       " ['may i speak to you for a moment ? ', 'cannot it wait ? '],\n",
       " ['you know what ? ', 'what ? '],\n",
       " ['what are you doing ? ', 'flying the plane . '],\n",
       " ['where are we sending it ? ', 'white house situation room . '],\n",
       " ['what is going on ? ', 'we are under attack . '],\n",
       " ['one . ', 'but . . . '],\n",
       " ['who can i say is calling ? ', 'this is the president . '],\n",
       " ['you give me ulcers . ', 'that is my job . '],\n",
       " ['what ? ', 'i just wish it was that simple . '],\n",
       " ['forget it .  i was reading . ', 'i was reading too . '],\n",
       " ['did you feel that ? ', 'yes i did . . . '],\n",
       " ['ted , what is wrong ? ', 'ask simon . '],\n",
       " ['ted , we are taking off ! ', 'let me by , elaine . '],\n",
       " ['ted . ', 'not now , elaine ! '],\n",
       " ['elaine ! ', 'ted ! '],\n",
       " ['elaine , what is going on ? ', 'ted , there is no time to explain . '],\n",
       " ['ted , we have only got ten minutes . ', 'not now , elaine . '],\n",
       " ['now ! ', 'compute ! '],\n",
       " ['frank is the best pilot in the program . ', \"i ' m so excited , simon . \"],\n",
       " ['i guess this is a first for you . ', \"no , i ' ve been excited before . \"],\n",
       " ['have you got it straightened out now ? ', 'i think so . '],\n",
       " ['can i hold him ? ', 'sure . '],\n",
       " ['he is a boy dog . ', 'yeah . '],\n",
       " ['he is carrying a bomb . ', 'a b . . . '],\n",
       " ['i have to see bud kruger . ', 'do you have an appointment , sir ? '],\n",
       " ['you cannot go in there ! ', 'do not try to stop me ! '],\n",
       " ['ah , will not you sit down ? ', 'thank you . cream ? '],\n",
       " ['captain , how soon can we land ? ', 'i cannot tell . '],\n",
       " ['what did he have ? ', 'he had fish . '],\n",
       " ['elaine ! ', 'ted ! '],\n",
       " ['rain . ', 'and a little ice . '],\n",
       " [\"rats ! i ' ve lost number three . \",\n",
       "  'what happened , ted ? what went wrong ? '],\n",
       " ['ted . . . ', 'yes ? '],\n",
       " ['see them , elaine ? ', 'uh huh . '],\n",
       " ['we have a visitor . ', 'hello . '],\n",
       " ['oh , i cannot stand it . ', 'what is it ? '],\n",
       " ['hi ! ', \"i ' m randy . \"],\n",
       " [\"oh , i ' d love to . \", 'okay , this is one of my favorites . '],\n",
       " ['roger . ', 'huh ? '],\n",
       " ['roger . ', 'huh ? '],\n",
       " ['excuse me , sister ? ', 'yes ? '],\n",
       " ['you okay ? ', 'yeah . . . '],\n",
       " ['in there . ', 'show me . '],\n",
       " ['kill them both . ', 'here ? '],\n",
       " ['get the picture ? ', 'and that fits ? '],\n",
       " ['why did you do it ? ', \"why ' d i do what ? \"],\n",
       " ['who said that ? ', 'at the end of the bar . '],\n",
       " ['back off , george . ', 'but i . '],\n",
       " ['screw you . ', 'screw me ?  that cannot be right . '],\n",
       " ['tell me . ', 'your mother mates out of season . '],\n",
       " ['matthew , you do not have to . ', \"stay back !  i ' m okay . \"],\n",
       " ['who is he ? ', 'todd watson .  the assistant manager . '],\n",
       " ['well , let is roll , george . ', 'to the . . . to the beach ? '],\n",
       " ['stop the car . ', 'why ? '],\n",
       " ['what was that about ? ', 'nothing . '],\n",
       " ['we ?  you have taken it ? ', 'we all did . '],\n",
       " ['no !  we must do this alone . ', 'do what ? !  george ? ! '],\n",
       " ['how do i look ? ', 'you look very good . '],\n",
       " ['does that look at all suspicious to you ? ',\n",
       "  'whatever gave you that idea ? '],\n",
       " ['get outta there ! ', 'i cannot !  do you mind ! '],\n",
       " ['cannot lock up . . . ', 'talk to me , hudson . '],\n",
       " [\"i ' ll go . \", 'what ? '],\n",
       " ['how much time ? ', 'plenty !  twenty six minutes ! '],\n",
       " ['you did okay , bishop . ', 'well , thanks , i  '],\n",
       " ['no .  there is no way ! ', 'hear me out . . . '],\n",
       " ['you son of a bitch . ', 'do not make me pull rank , ripley . '],\n",
       " ['what is it ? ', 'i do not know . '],\n",
       " ['i told them to fall back . . . ', 'they are but off !  do something ! '],\n",
       " ['yes , hicks ? ', 'hudson , sir .  he is hicks . '],\n",
       " ['save it . ', 'sure , hicks . '],\n",
       " ['let is get the fuck out of here ! ', 'not that tunnel , the other one ! '],\n",
       " ['the corner !  ready ? ', 'do it ! '],\n",
       " ['seventeen meters . ', 'let is get these things lit . '],\n",
       " ['let is go !  let is go ! ', \"fuckin ' a ! \"],\n",
       " ['they are in the approach corridor . ', 'on my way . '],\n",
       " ['they will get us . ', 'maybe .  maybe not . '],\n",
       " ['locked . ', 'stand back . '],\n",
       " ['hicks , do not let him leave . ', 'we are not going anywhere . '],\n",
       " ['ellen . ', 'do not be long , ellen . '],\n",
       " ['it is inside the complex . ', 'you are just reading me . '],\n",
       " ['thanks . ', 'uh , what is next ? '],\n",
       " ['range twenty meters . ', 'seal the door . '],\n",
       " ['and how are we today ? ', 'terrible . '],\n",
       " ['i do not want you for a friend . ', 'why not ? '],\n",
       " ['cross my heart . ', 'and hope to die ? '],\n",
       " ['yes , there are , are not there . ',\n",
       "  'why do they tell little kids that ? '],\n",
       " ['no , it is different , honey . ', 'did you ever have a baby ? '],\n",
       " ['yes .  a little girl . ', 'where is she ? '],\n",
       " ['gone . ', 'you mean dead . '],\n",
       " ['burke !  open the door ! ', 'look ! '],\n",
       " ['mommy . . . mommy ? ', 'right here , baby .  right here . '],\n",
       " ['are we going to sleep now ? ', 'that is right . '],\n",
       " ['well , i heard you met herr mozart . ',\n",
       "  'oh ? news travels fast in vienna . '],\n",
       " ['is there a part for me ? ', 'no . '],\n",
       " ['what does he look like ? ', 'you might be disappointed . '],\n",
       " ['did you know ? had you heard ? ', 'what ? '],\n",
       " ['the marriage ! ', 'well , what does it matter to you ? '],\n",
       " ['how was i ? tell me honestly . ', 'you were sublime . '],\n",
       " ['what ? ', 'do not bother . '],\n",
       " ['oh  excuse me ! ', 'is her mother still lying on the floor ? '],\n",
       " ['no , she is fine . ', \"i ' m so relieved . \"],\n",
       " ['is she a good fuck ? ', 'what ? ? '],\n",
       " ['what is it ? ', 'i want to go ! '],\n",
       " ['where ? ', 'i want to go back to vienna . '],\n",
       " ['now ? ', 'yes ! '],\n",
       " ['yes . ', 'eat my  ah ! '],\n",
       " ['yes ! ', 'well , do not do that again ! '],\n",
       " ['what is the matter with you ? ', 'tell them to go ! '],\n",
       " ['sssh . what is it ? tell me . ', 'no ! '],\n",
       " ['yes ! ', 'i love you ! i love you ! '],\n",
       " ['there is a young girl to see you . ', 'what does she want ? '],\n",
       " ['i do not know . ', 'well , ask her ! '],\n",
       " ['what ? ', 'your father is dead . '],\n",
       " ['you are not going to do this ? ', 'why not ? half the house ! '],\n",
       " ['who was that ? ', 'no one . '],\n",
       " ['why not ? ', 'you would think i was mad . '],\n",
       " ['no . do not answer it ! ', 'why ? '],\n",
       " ['give me one reason i can understand . ', 'i cannot write it ! '],\n",
       " ['why not ? ', 'it is killing me . '],\n",
       " ['excellency ! ', 'madame . how can i help you ? '],\n",
       " ['originals ? ', 'yes . '],\n",
       " ['come back tonight . ', 'tonight ? '],\n",
       " ['alone . ', 'what for ? '],\n",
       " ['why you ? ', 'i was at hand . '],\n",
       " ['am i interrupting something ? ', 'not at all . '],\n",
       " ['i see that you are expecting . ', 'oh , yes . '],\n",
       " ['when , may i ask ? ', 'in three months ! papa . '],\n",
       " ['be careful ! ', 'be careful ! '],\n",
       " ['he is adorable ! ', 'adorable ! '],\n",
       " ['behold ! ', 'behold ! '],\n",
       " ['hey ! ', 'hey ! '],\n",
       " ['behold ! ', 'behold ! '],\n",
       " ['i said play ! ', 'michael ! '],\n",
       " ['what a strange young man . ', 'yes . he is a little strange . '],\n",
       " ['really ? ', 'ah , now ! here she comes . '],\n",
       " ['just one year . ', 'who was your teacher ? '],\n",
       " ['may i try it ? ', 'majesty . '],\n",
       " ['a flat , majesty . ', 'ah ha ! '],\n",
       " ['an interesting idea , majesty . but  ', 'yes ? '],\n",
       " ['sire . ', 'well . there it is . '],\n",
       " ['excuse me , but how old are you ? ', 'twenty six . '],\n",
       " ['yes , sir . ', 'well . there it is . '],\n",
       " ['yes , what about him ? ', 'he is here . '],\n",
       " ['why are you here ? ', 'am i not welcome ? '],\n",
       " ['it could not be better . ', 'that is not what i hear . '],\n",
       " ['pupils ? ', 'yes . '],\n",
       " ['yes . ', 'how many ? '],\n",
       " ['what is that ? ', 'oh , let is not talk about it . '],\n",
       " ['why not ? ', 'it is a secret . '],\n",
       " ['no , really ! ', 'this is just a game , papa . '],\n",
       " ['papa , is this your idea ? ', 'mine ? '],\n",
       " ['oh , thank you , sir . ', 'do any pupils come to the house ? '],\n",
       " ['where does he work ? ', 'in there , sir . '],\n",
       " ['what medicine ? ', 'i do not know . he has pains . '],\n",
       " ['yes ? ', 'are you herr mozart ? '],\n",
       " ['put it down ! ', 'what is this ? '],\n",
       " ['yes . ', 'can i see it ? '],\n",
       " ['no . ', 'why not ? '],\n",
       " ['do you have a daughter ? ', \"i ' m afraid not . \"],\n",
       " ['with all my heart , mozart . ', 'thank you ! oh , thank you . '],\n",
       " ['did my work please you ? ', 'how could it not , excellency ? '],\n",
       " ['a mass for the dead . ', 'what dead ? who is dead ? '],\n",
       " ['are you ill ? ', 'the doctor thinks i am . but  '],\n",
       " ['shall i answer it ? ', 'no ! no , it is him ! '],\n",
       " ['who ? ', 'the man . he is here . '],\n",
       " ['do you believe in it ? ', 'what ? '],\n",
       " ['strange ! ', 'come . let is begin . '],\n",
       " ['yes . ', 'so now  a minor . suddenly . '],\n",
       " ['the fire . ', 'what time ? '],\n",
       " ['do you have me ? ', 'i think so . '],\n",
       " ['do you have it ? ', 'yes . '],\n",
       " ['i am so ashamed . ', 'what for ? '],\n",
       " ['you mean in turkey ? ', 'exactly . '],\n",
       " ['that will do , herr mozart ! ', 'just let me tell you how it begins . '],\n",
       " ['well done , mozart . really quite fine . ', 'baron ! '],\n",
       " ['leave me alone . ', 'i cannot leave alone a soul in pain . '],\n",
       " ['where ? ', 'here in vienna . '],\n",
       " ['well ? ', 'i regret it is not too familiar . '],\n",
       " ['all vienna has heard that . ', 'and do they believe it ? '],\n",
       " ['is it true ? ', 'do you believe it ? '],\n",
       " ['but not his body . ', 'what difference does that make ? '],\n",
       " ['you mean that play ? ', 'exactly . '],\n",
       " ['he is setting that play to music ? ', 'yes . '],\n",
       " ['where ? ', 'never mind . '],\n",
       " ['bravo , your majesty ! ', 'well done , sire ! '],\n",
       " ['here i am , my angel . ', 'what ? who the devil are you ? '],\n",
       " ['well , promise then . ', 'what do you mean  now ? '],\n",
       " ['that lady is back , sir . ', 'show her in . then go to bed . '],\n",
       " ['oh , miss price ? ', 'yes , doctor ? '],\n",
       " ['what exactly did he call out ? ', \"he said ' jack ' . \"],\n",
       " ['did he say a wolf ? ', 'yes , i believe he did . '],\n",
       " ['yes . ', 'what did he say ? '],\n",
       " ['david says jack comes to warn him . ', 'warn him ? '],\n",
       " ['could you get here without any trouble ? ', 'yes , i should think so . '],\n",
       " ['shall i send a car ? ', 'no , a cab will be faster . '],\n",
       " ['what shall we do ? ', 'tea would be nice . '],\n",
       " ['it was not a lunatic . ', 'i beg your pardon ? '],\n",
       " ['it was a wolf . ', 'what ? '],\n",
       " ['mr . kessler ? ', 'yes ? '],\n",
       " ['aw come on , miss price ! ', 'call me alex . '],\n",
       " ['you are a very beautiful girl . ', 'i thought you were asleep . '],\n",
       " ['what do you dream about ? ', 'i dream of death mostly . '],\n",
       " ['how old are you ? ', 'that is not really a very proper question . '],\n",
       " ['how old are you ? ', 'twenty eight . '],\n",
       " [\"i ' m twenty seven . \", 'i know . '],\n",
       " ['my best friend . my very best friend . ', 'shall i read to you ? '],\n",
       " ['hello . you all right ? ', \"i ' m sorry i woke you up . \"],\n",
       " [\"i ' m a werewolf . \", 'a werewolf ? '],\n",
       " ['my friend jack was just here . ', 'your dead friend jack ? '],\n",
       " ['i was dreaming again ? ', 'i would think so . '],\n",
       " ['the kitchen . ', 'very nice . '],\n",
       " ['closet . ', 'charming . '],\n",
       " ['bathroom . ', 'lovely . '],\n",
       " ['the bedroom . ', 'there is only one bed . '],\n",
       " ['it is nice to see you . ', 'it is nice to see you . '],\n",
       " ['alex ? ', 'yes ? '],\n",
       " ['will you be here in about fifteen minutes ? ', 'of course . '],\n",
       " ['but , david . ', 'i was not hallucinating . '],\n",
       " ['the next corner we can get a cab . ', 'i should be committed . '],\n",
       " ['but . . . ', 'pull over . '],\n",
       " ['he is playing a stupid joke , sir . ', 'what ? '],\n",
       " ['hopeless . it is hopeless . ', 'david , let is go now . '],\n",
       " ['hello , benjamin . ', 'no . '],\n",
       " ['no what ? ', 'no . '],\n",
       " ['feeling better ? ', 'no . '],\n",
       " ['how are we feeling tonight ? ', 'no . '],\n",
       " ['no what ? ', 'no ! '],\n",
       " ['i think he is a jew . ', 'why on earth do you say that ? '],\n",
       " ['officer , i killed those people last night . ', 'you did , did you ? '],\n",
       " ['mr . kessler ? wake up , please . ', 'i was having a nightmare . '],\n",
       " ['there were witnesses ? ', 'so they said . '],\n",
       " ['are you cold ? ', 'yes . '],\n",
       " ['jack . ', 'david . '],\n",
       " ['okay . ', 'shall we ? '],\n",
       " ['hello . ', 'nice to see you . '],\n",
       " ['ask them what the candles are for . ', 'you ask them . '],\n",
       " ['right . ', 'wrong . '],\n",
       " ['go on , ask them . ', 'you ask them . '],\n",
       " ['what do you think was wrong ? ', 'i have no idea . '],\n",
       " ['did you hear that ? ', 'i heard that . '],\n",
       " ['could be a lot of things . ', 'yeah ? '],\n",
       " ['shit ! david , what is that ? ', 'i do not know . come on . '],\n",
       " ['what is the plan ? ', 'plan ? '],\n",
       " ['it is in front of us . ', 'do you think it is a dog ? '],\n",
       " ['nice doggie . good boy . ', 'walk away , jack . '],\n",
       " ['see anything ? ', 'no . '],\n",
       " ['it sounds far away . ', 'not far enough . come on . '],\n",
       " ['jack ? ', 'yeah . '],\n",
       " ['you really scared me , you shithead . ', 'are you going to help me up ? '],\n",
       " ['nice to see you . ', 'get the fuck out of here , jack . '],\n",
       " [\"i ' m going completely crazy . \", 'david ! '],\n",
       " ['it is you , david . ', 'what ? ! '],\n",
       " ['what are you doing here ? ', 'i wanted to see you . '],\n",
       " ['jack , are you really dead ? ', 'what do you think ? '],\n",
       " ['hi , jack . ', 'hi , david . '],\n",
       " ['what can i say , jack ? ', 'you do not have to say anything . '],\n",
       " ['you look awful . ', 'thank you . '],\n",
       " ['because this must be stopped . ', 'how shall i do it ? '],\n",
       " ['what here ? ', 'he is on the telephone . '],\n",
       " ['do you have any hot soup ? ', 'no . '],\n",
       " ['then you have some hot tea ? ', 'no . '],\n",
       " ['no , thank you . ', \"i ' d like some tea , please . \"],\n",
       " ['hello , tom . ', 'you here again ? what do you want ? '],\n",
       " ['where is matt ? ', 'matt ? '],\n",
       " ['he has ? ', 'yeah . '],\n",
       " ['about twenty four thousand . ', 'it was more than that last week . '],\n",
       " ['yeah . ', 'here is twenty five thousand . '],\n",
       " ['charlie ! ', 'yeah ? '],\n",
       " ['i remember once when your account checked . ', 'yeah . '],\n",
       " ['mr . dickson in yet ? ', 'not yet , mr . clark . '],\n",
       " ['we want to talk to you . ', 'what about ? '],\n",
       " ['oh ! ', 'you understand ? '],\n",
       " ['no . ', 'i want to be near you ! '],\n",
       " ['what is this ? ', 'my apartment . '],\n",
       " ['what is that , mr . bones ? ', 'a misunderstood bachelor . '],\n",
       " ['why , matt ! ', 'what are you doing here ? '],\n",
       " ['good morning , mr . dickson . ', 'john , how is your wife this morning ? '],\n",
       " ['much better this morning , thank you . ', 'got a handkerchief ? '],\n",
       " ['in mr . sampson is office . ', 'now do not you worry about it . '],\n",
       " ['good morning , helen . ', 'good morning . '],\n",
       " ['yes ? ', 'mr . sampson . . . '],\n",
       " ['helen ! ', 'yes ? '],\n",
       " ['got my letter ? ', 'yes , thank you . '],\n",
       " ['wait a minute . where is your uniform ? ', 'i have not any . '],\n",
       " ['you have not got a uniform ? ', 'no , sir . '],\n",
       " ['oh , make that uniform blue . ', 'yes , sir . '],\n",
       " ['i already told him i was home . ', 'there you are . '],\n",
       " ['no . i will not . ', 'you are protecting somebody . '],\n",
       " ['yes , sir . ', 'mine too ? '],\n",
       " ['how do you do , mrs . dickson . ', 'is that busy husband of mine busy ? '],\n",
       " ['hello , helen ! ', 'matt , come here ! '],\n",
       " ['why ? ', 'come here , honey ! '],\n",
       " ['what did you do with it ? ', 'with what ? '],\n",
       " ['the ten dollars . ', 'oh , ten dollars '],\n",
       " ['no . ', \"oh , you think i ' m lying ? \"],\n",
       " ['well . . . ? ', 'well , he was making love to her . '],\n",
       " ['what is keeping you ? ', 'oh , charlie again . '],\n",
       " ['he was in the bank yesterday . ', 'he was here ? '],\n",
       " ['what time was it ? ', 'huh ? '],\n",
       " ['then who changed it ? ', 'i do not know . '],\n",
       " ['so you were home last night ? ', 'yes . '],\n",
       " ['how are you fixed ? ', \"i ' m okay , matt . \"],\n",
       " ['everybody in ? ', 'i guess so . '],\n",
       " ['i could not lose him . ', 'jim younger , i told you '],\n",
       " ['this is healing ? ', 'sometimes a wound will kill . '],\n",
       " ['he is smiling . ', 'never a good thing . '],\n",
       " ['uhh , yeah it does . ', 'you stay out of this , bob . '],\n",
       " ['how did they  ', 'what have you done ? '],\n",
       " ['i are not done  ', 'what have you done ? ! '],\n",
       " ['bob . i did not . . . ', 'swear . '],\n",
       " ['i swear  ', 'swear on jimmy is grave . '],\n",
       " ['the army can hang him . ', 'tomorrow . '],\n",
       " ['how many of them did he kill ? ', 'two . '],\n",
       " ['you have no shame . ', \"not yet . but i ' m hoping . \"],\n",
       " ['he is planning a job . ', 'what ? '],\n",
       " [\"what you sayin ' boy ? \", 'i think i recognize you . '],\n",
       " [\"' bout time you got here , buddy . \", 'what is going on ? '],\n",
       " ['ride with me , cousin ? ', 'i could use the walk . '],\n",
       " ['hands off your hip , cole . ', 'you are not scared , are you ? '],\n",
       " ['not now . ', 'what is wrong with you ? '],\n",
       " ['the james younger gang . ', 'sorry . '],\n",
       " ['i got seven thousand . ', 'i got three . '],\n",
       " ['jesse , we got to have a word . ', 'sure , cousin . '],\n",
       " ['it will be the biggest score yet . ', 'what will be ? '],\n",
       " ['a bad idea . ', 'i got us through the war all right . '],\n",
       " ['dammit ! ', 'a trap . '],\n",
       " ['we will make them pay for this . ', \"i ' m out . \"],\n",
       " ['missed you , cousin . ', 'missed you too , cousin . '],\n",
       " ['well ? ', \"i ' m thinking . . . \"],\n",
       " [\"i think one of ' em is glass . \", 'which one , right or left ? '],\n",
       " ['another dozen out back . ', 'they gonna rush us ? '],\n",
       " ['the safe . now . ', 'of course !  uh , sir ? '],\n",
       " ['what ? ', 'where is jesse james ? '],\n",
       " ['then why cannot i go in there ? ', 'on account of we are robbing it . '],\n",
       " [\"fine . i ' ll just wait here . \", \"i ' d appreciate that . \"],\n",
       " ['what the  ', 'what is it ? '],\n",
       " ['they are gone . what are you  ',\n",
       "  'i fooled them into thinking i was alone . '],\n",
       " ['that would have made an impression . ', 'i figure . '],\n",
       " ['that just might work . ', 'maybe , maybe . . . '],\n",
       " ['big and older ? ', 'you can shut up now . '],\n",
       " ['you know him ? ', 'heard of him . '],\n",
       " ['how did it go in there ? ', 'fine . how did it go out here ? '],\n",
       " ['we are drunk . ', 'oh yeah . '],\n",
       " [\"y ' know , uncle frank . . . \", 'yeah , jimmy ? '],\n",
       " ['that he was . ', 'they are making him a hero now . '],\n",
       " ['uncle frank ? ', 'yeah jimmy ? '],\n",
       " ['how much of that story is true ? ', 'everything but the boring parts . '],\n",
       " ['ma ! please ! ', 'boys ? '],\n",
       " ['jesse , are you awake ? ', 'mmmm . '],\n",
       " ['not if i find the right girl . ', 'and what is this right girl like ? '],\n",
       " ['hmm . ', 'hmm what ? '],\n",
       " ['do not turn around . ', 'what ? '],\n",
       " ['do not swear . ', 'yes madam . '],\n",
       " ['we are moving you tomorrow . ', 'but i like the presidential suite . '],\n",
       " ['that is what i would have done . ', \"i ' m not hanged yet . \"],\n",
       " ['now let is have a drink . ', 'right here in church ? '],\n",
       " ['i like that . ', 'no . '],\n",
       " ['well , not exactly . ', 'you been with a girl ever ? '],\n",
       " ['not at all . ', 'i just though you were awful cute . '],\n",
       " ['there is only four of them . . . ', 'move you fools ! '],\n",
       " ['the final route for the railroad is complete . ',\n",
       "  'i look forward to seeing it . '],\n",
       " ['parker . ', 'sir ? '],\n",
       " ['what is that ? ', 'what , sir ? '],\n",
       " ['this is him . ', 'i remember you . '],\n",
       " ['so he is won . ', 'no . '],\n",
       " ['yes ? ', 'pumpkin you are dating an asshole . '],\n",
       " ['i cannot . ', \"i ' m thinking dorsia . \"],\n",
       " ['dorsia is nice . ', 'nice ? '],\n",
       " ['okay . yeah . what time ? ', 'eight ? '],\n",
       " ['are we here ? ', 'yes . '],\n",
       " ['this is dorsia ? ', 'yes , dear . '],\n",
       " ['it is a plain end . i think . ', 'turn the light on . '],\n",
       " ['will you call me before thanksgiving ? ', 'maybe . '],\n",
       " ['that is nice . ', 'you and . . . luis ? '],\n",
       " ['patrick ? ', 'yes ? '],\n",
       " ['i have not seen you around here . ', 'you just have not been looking . '],\n",
       " ['nothing like last time , promise . ', 'alright . '],\n",
       " ['this is nicer than your other apartment . ', 'it is not that nice . '],\n",
       " ['get married . have a wedding . ', 'evelyn ? '],\n",
       " ['we should do it . ', 'no i cannot take the time off work . '],\n",
       " ['he is rich . ', 'everybody is rich . '],\n",
       " ['he is good looking . ', 'everybody is good looking , patrick . '],\n",
       " ['he has a great body ', 'everybody has a great body now . '],\n",
       " ['you are really serious , are not you ? ', 'yes , i am . '],\n",
       " ['but what about the past ? our past ? ', 'we never really shared one . '],\n",
       " ['where are you going ? ', \"i ' m just leaving . \"],\n",
       " ['are you my two of the clock ? ', 'no . '],\n",
       " ['does not he live here ? ', 'no , he does not . '],\n",
       " ['call me please , patrick . ', 'jesus lives , luis . '],\n",
       " ['shut up ! ', 'calm down . let is do it anyway '],\n",
       " ['we are totally booked . ', 'oh really ? that is great . '],\n",
       " ['i said we are totally booked . ', 'two at nine ? perfect . '],\n",
       " ['after six . ', 'negative . cancel it . '],\n",
       " ['what is it ? ', 'patrick ? '],\n",
       " ['who ? ', 'detective donald kimball ? '],\n",
       " ['patrick ? ', 'can you bring mr . . . '],\n",
       " ['jean ? ', 'yes , patrick ? '],\n",
       " ['oh patrick , i cannot make this decision . ',\n",
       "  'no , come on . anywhere you want . '],\n",
       " ['do you have a boyfriend ? ', 'no , not really . '],\n",
       " ['forget it . ', 'what is that ? '],\n",
       " ['jean ? what ? ', 'make someone happy have you ever wanted to ? '],\n",
       " ['patrick ? is that you ? ', 'hello ? jean , i need help ! '],\n",
       " ['where are you ? ', \"jean i ' m not \"],\n",
       " ['patrick ? i cannot hear you . ', 'what are i doing ? '],\n",
       " ['. . . to the office this afternoon . ', 'why ? '],\n",
       " ['so , what do you do ? ', 'what do you think i do ? '],\n",
       " ['she is my . . . cousin . ', 'uh huh ? '],\n",
       " ['do not tell him you are here . ', 'why would i ? '],\n",
       " ['who ? ', 'paul owen . '],\n",
       " ['coffee ? ', \"no . i ' m okay . \"],\n",
       " [\"oh no , i ' m okay . \", 'it is no problem '],\n",
       " ['where did you go to school ? ', 'harvard . the harvard business school . '],\n",
       " ['nice . very nice . ', 'thanks . '],\n",
       " ['pardon me , but are you okay ? ', 'who do you ask ? '],\n",
       " ['bad habit . ', \"i know . i ' m sorry . \"],\n",
       " ['would you rather i not smoke ? ', 'no , i guess it is okay . '],\n",
       " ['you sure ? ', 'no problem . '],\n",
       " ['what can you tell me about paul owen ? ', 'well . . . '],\n",
       " ['do you feel that way ? ', 'no . not really . '],\n",
       " ['where did paul hang out ? ', 'hang . . . out ? '],\n",
       " ['anything else you can tell me about owen ? ', 'we were both seven in . '],\n",
       " ['so was i . ', 'do you have any witnesses or fingerprints ? '],\n",
       " ['actually , yes . ', 'hmmm . '],\n",
       " ['nothing . ', 'people just . . . disappear . '],\n",
       " ['he does ? you are sure ? ', 'i checked it out . it is clean . '],\n",
       " ['oh . kimball now where were you ? ', 'where was marcus ? '],\n",
       " ['he was not with paul owen . ', 'so who was he with ? '],\n",
       " ['gee , uh , that is too bad . ', \"i ' m so hungry . \"],\n",
       " ['listen , what is your name ? ', 'al . '],\n",
       " ['speak up . come on . ', 'al . '],\n",
       " ['you do not wanna move , do you ? ', 'i can paint anywhere . '],\n",
       " ['that took some fun out of  ', 'we are not gonna let it . '],\n",
       " ['what ! ', 'teddy was killed last night . '],\n",
       " ['what are you  what ? ', 'it was a hate crime . '],\n",
       " ['you think i do not know that ? ', 'milo . why would he  '],\n",
       " ['i cannot just walk away ! ', 'you cannot just walk in , either . '],\n",
       " ['that was  different . ', '. . . different ? '],\n",
       " ['look at this . ', 'what ? '],\n",
       " ['you look beautiful . ', 'yeah ? give me a goodbye kiss . '],\n",
       " ['here we go . ', 'great . '],\n",
       " ['what ? ', \"what ' d they ever do for you ? \"],\n",
       " ['what ? ! ', 'wanna be a part of history ? '],\n",
       " ['milo . surprised he is not your guest . ', 'we tried ! '],\n",
       " [\"milo ? i ' m danny . \", 'oh hi . '],\n",
       " ['cool ! ', 'would you like a coke or something ? '],\n",
       " [\"' think i should buy some originals ? \", '. . . do i ? '],\n",
       " ['how is it going ? ', \"maybe i ' m going too fast . \"],\n",
       " ['you did this  overnight ? ', 'you are making me young again . '],\n",
       " ['milo . what is up ? ', 'well  you sent for me . '],\n",
       " ['you really wrote this just today ? ', 'what are you implying . '],\n",
       " ['did you ? ', 'talk about work ? never ! '],\n",
       " ['i meant did you find other stuff to  ', 'oh . yeah . '],\n",
       " ['gary , hi . ', 'you look a little tired . '],\n",
       " ['gary , i  ', 'you see what is hanging on the wall ? '],\n",
       " ['ready for number three ? ', 'let is go . '],\n",
       " ['gary ? ', 'just do it . '],\n",
       " ['i think you should go . ', 'you do ? '],\n",
       " ['. . . you wanna work  here ? ', 'got out of my other commitment . '],\n",
       " ['you calling phil and randy ? ', \"i ' m calling gary . \"],\n",
       " ['come with me . ', 'where we going ? '],\n",
       " ['lisa . ', 'you know my name . '],\n",
       " ['you know mine . ', 'you are famous around here . '],\n",
       " ['did you wanna be alone ? ', 'no . please . '],\n",
       " ['. . . milo ? ', 'hmm ? '],\n",
       " ['how close are you ? ', 'what ? '],\n",
       " ['i told teddy about you . ', \"what ' d he say ? \"],\n",
       " ['bring it to the other location . ', 'but you said the other  '],\n",
       " ['maybe it is the satellite . ', 'let is try . '],\n",
       " ['. . . he knows . ', 'what ? '],\n",
       " ['is that how larry feels ? ', 'uh . not exactly . '],\n",
       " ['what ? come on . is it serious ? ', 'i do not know . '],\n",
       " ['maybe he will get back to work . ', 'speaking of which . . . '],\n",
       " ['that kid is the great white hope . ', 'i could get it out of him . '],\n",
       " [\"who ' re these guys ? \", 'where is he ? '],\n",
       " ['come in . ', \"i ' d rather talk about number two . \"],\n",
       " ['may i wash you ? ', 'groovy . '],\n",
       " ['to my health . ', 'kiss me . '],\n",
       " ['thank you . ', 'oh , and austin . . . '],\n",
       " ['yes ? ', 'be careful . '],\n",
       " ['oh , and austin . . . ', 'yes ? '],\n",
       " ['be careful . ', 'thanks . '],\n",
       " ['oh , and austin . . . ', 'yes ? '],\n",
       " ['thank you , basil . ', 'oh , and austin . . . '],\n",
       " ['oh , yes . thank you . ', 'there is one more thing , austin . '],\n",
       " ['yes ? ', 'be careful . '],\n",
       " ['be careful . ', 'thanks . '],\n",
       " ['that does not compute . ', 'why not ? it is a question . '],\n",
       " ['do i make you horny ? ', 'what ? '],\n",
       " ['you are smashed , vanessa . ', 'i am not . '],\n",
       " ['who ? ', 'you know who . '],\n",
       " ['does that make you horny ? ', 'not now , austin . '],\n",
       " ['i have something to tell you . ', 'lay it on me . '],\n",
       " ['kiss me . ', 'behave ! '],\n",
       " ['danger powers , personal effects . ',\n",
       "  'actually , my name is austin powers . '],\n",
       " ['it says here , name danger powers . ', 'danger is my middle name . '],\n",
       " ['everything seems to be in order . ', 'hey , wait a minute '],\n",
       " ['here , have a piece of gum . ', 'here , have a piece of gum . '],\n",
       " ['ow ! you shot me ! ', 'right . okay . moving on . '],\n",
       " ['quite impressive . ', 'thank you , herr doctor . '],\n",
       " ['what did you say ? ', 'show me . '],\n",
       " ['how is austin ? ', 'he is asleep . '],\n",
       " [\"i ' m proud of you . \", 'why ? '],\n",
       " ['red . . . ', '. . . head . . . '],\n",
       " ['knight . . . ', 'black . . . '],\n",
       " ['. . . death . . . ', 'love . . . '],\n",
       " ['flower . . . ', '. . . power . . . '],\n",
       " ['secret . . . ', '. . . love . . . '],\n",
       " ['hope . . . ', '. . . love . . . '],\n",
       " ['fear . . . ', '. . . love . . . '],\n",
       " ['how long have i been here ? ', 'three days . '],\n",
       " ['you ', 'have we met ? '],\n",
       " ['ah , beautiful . just as he promised . ', 'promised ? who promised ? '],\n",
       " ['may i help you , madam . . . ', 'mr . john steed , please . '],\n",
       " [\"i ' m afraid that is impossible . \", 'impossible ? '],\n",
       " ['you are female ? ', 'as you see . '],\n",
       " ['then you cannot come in . ', 'i have an appointment . '],\n",
       " ['people , too . ', 'then who wins ? '],\n",
       " ['what are you trying to do to me ? ', 'we want to help . . . ! '],\n",
       " ['of course . i planned that , too . ', 'but  why ? '],\n",
       " ['i did not . mother did . ', 'mother ? '],\n",
       " ['do we always follow mother is instructions ? ',\n",
       "  'for a man in my position  '],\n",
       " ['no thanks . ', 'i meant me . '],\n",
       " ['what on earth ? ', 'any ideas ? '],\n",
       " ['not quite . this is my field . ', 'is there anything that is not ? '],\n",
       " ['steed . . . ! ', 'mrs . peel . . . ? '],\n",
       " ['you followed me . ', 'orders . '],\n",
       " ['to kill me ? ', 'nothing personal . '],\n",
       " ['i could save you the trouble . ', 'no trouble . '],\n",
       " ['mmm . . . what are you doing ? ', 'keeping a stiff upper lip ? '],\n",
       " ['but you did suspect me . ', 'not for a moment . '],\n",
       " ['you are playing games . ', 'are not we all , mrs . peel ? '],\n",
       " ['i thought you played by the rules . ', 'i thought you did not . '],\n",
       " [\"i ' m playing to win . \", 'winning is not everything . '],\n",
       " ['no , after you . ', 'you do not trust me ? '],\n",
       " ['i told mother i took care of you . ', 'you lied . '],\n",
       " [\"i ' ll be back . . . \", 'where are you going ? '],\n",
       " ['mrs . peel ? ', 'what kept you ? '],\n",
       " ['i was getting to it . ', 'getting to what ? '],\n",
       " ['very good , mrs . peel . . . ', 'i shall need a small plane . '],\n",
       " ['oh , hello . . . ', 'we want mrs . peel . '],\n",
       " ['steed ', 'how did you guess ? '],\n",
       " ['amnesia ? ', 'possibly . split personality . . . '],\n",
       " ['why ? ', 'she may try to kill you . '],\n",
       " ['you with mother or father ? ', 'both , actually . '],\n",
       " ['bang bang . . . you are dead . ', 'you wish . '],\n",
       " ['one shot  for emergencies . ', 'that is not playing by the rules . '],\n",
       " ['rules are made to be broken . ', 'if you say so . '],\n",
       " ['you said . . . one shot . ', 'did i ? my mistake . '],\n",
       " ['are not you forgetting about something ? ',\n",
       "  'you are , and it is behind you . '],\n",
       " ['what is happening ? ', 'debbie is marrying rick . '],\n",
       " ['what ? ', 'you are kidding . '],\n",
       " ['all right ! ', 'when do the girls get to the party ? '],\n",
       " ['and . . . ', 'bond . . . james bond . '],\n",
       " ['seventy five hundred . ', 'not interested . '],\n",
       " ['i do not want any trouble . ', 'oh , come on , just a little . '],\n",
       " ['great car . ', 'the best . '],\n",
       " ['i love that car . ', \"i ' m very happy for you two . \"],\n",
       " ['cole , what the hell are you doing ? ', 'she is mine ! '],\n",
       " ['nice shot . ', 'thank you , sir . '],\n",
       " ['thanks for the advise , sir . ', 'keep me informed . '],\n",
       " ['huh ? wha . . . ', 'i cannot sleep . '],\n",
       " ['that feels so great . ', 'good . . . '],\n",
       " ['this is it , lady . last stop . ', 'cannot i just go with you guys ? '],\n",
       " ['are you okay ? ', 'yeah . '],\n",
       " ['give him the works . ', 'that is more like it . '],\n",
       " ['what can i be doing for you ? ', 'you are a pimp ? '],\n",
       " ['is that all the coke in the place ? ', 'that is it . '],\n",
       " ['you okay ? ', 'yeah , i guess so . '],\n",
       " ['welcome , welcome , one and all . ', 'rick ! '],\n",
       " ['mrs . ? ', \"i ' m separated . \"],\n",
       " ['where the hell is he ? ',\n",
       "  'knowing larry , he probably missed the flight . '],\n",
       " ['what is the matter ? ', 'nothing . . . let is get crazy ! '],\n",
       " [\"i ' m not really hungry . \", \"c ' mon . i insist . \"],\n",
       " ['what the hell is that ? ', 'my gift to you . '],\n",
       " ['under the table ! ', 'the best table in the house . '],\n",
       " ['i think you will enjoy this table . ', 'so long , father . '],\n",
       " [\"rick , i ' m concerned . \", 'about what ? '],\n",
       " ['who was that ? ', 'i do not know . '],\n",
       " ['what is this ? ', 'got me . '],\n",
       " [\"how ' bout this ? \", 'still drawing a blank . '],\n",
       " ['he look familiar ? ', 'very . '],\n",
       " ['now , do not get into any trouble . ', 'take care . '],\n",
       " ['hey , you guys . . . ', 'who is your friend ? '],\n",
       " ['guess who is here ? another surprise guest . ', 'who ? '],\n",
       " ['debbie . ', 'my debbie ? '],\n",
       " ['is he . . . ', 'he is alive . '],\n",
       " ['you gotta let me finish  ', 'just come down , john . just  '],\n",
       " ['it does not go like that . ', 'who asked you ? '],\n",
       " ['in the field museum once . ', 'it works . '],\n",
       " ['sheep do not count . ', 'yeah ? what about laura  '],\n",
       " ['thanks . ', 'brian    see ya tonight . '],\n",
       " ['hey . ', 'hey . '],\n",
       " ...]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_qa_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert words of the sentence to numbers --> This our vectorization\n",
    "def sentence2vector(voc, sentence):\n",
    "    vector = [voc.word2index[word] for word in sentence.split(' ')] + [EOS_TOKEN]\n",
    "    return vector \n",
    "\n",
    "def zero_padding(vectors, fillvalue=PAD_TOKEN):\n",
    "    return torch.LongTensor(list(itertools.zip_longest(*vectors, fillvalue=fillvalue))).t()\n",
    "\n",
    "def create_mask_matrix(lengths,max_length): # 1 means Value, 0 means Padding\n",
    "    mask_matrix = []\n",
    "    for length in lengths :\n",
    "        mask = sum([[0]*length,[1]*(max_length-length)],[])\n",
    "        mask_matrix.append(mask)\n",
    "    return  torch.BoolTensor(mask_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def questions2vectors(voc, questions):\n",
    "    \n",
    "    #Convert sentence to vector using words indexed\n",
    "    question_vectors = [sentence2vector(voc, sentence) for sentence in questions]\n",
    "    \n",
    "    #Get Length of each sentence\n",
    "    questions_lengths = torch.tensor([len(indexes) for indexes in question_vectors])\n",
    "    \n",
    "    #max_length can be 11 as we added EOS_TOKEN\n",
    "    max_length = max(questions_lengths) \n",
    "    \n",
    "    #Create mask matrix based on the lengths array\n",
    "    questions_mask = create_mask_matrix(questions_lengths, max_length)\n",
    "    \n",
    "    #Pad the sentences to the maximum length with PAD_TOKEN\n",
    "    question_vectors_padded = zero_padding(question_vectors)\n",
    "    \n",
    "    return question_vectors_padded, questions_mask, questions_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answers2vectors(voc, answers):\n",
    "    \n",
    "    #Convert sentence to vector using words indexed\n",
    "    answers_vectors = [sentence2vector(voc, sentence) for sentence in answers]\n",
    "    #Get Length of each sentence\n",
    "    answers_lengths = torch.tensor([len(indexes) for indexes in answers_vectors])\n",
    "    \n",
    "    #max_length can be 11 as we added EOS_TOKEN\n",
    "    max_length = max(answers_lengths) \n",
    "    \n",
    "    #Create mask matrix based on the lengths array\n",
    "    answers_mask = create_mask_matrix(questions_lengths, max_length)\n",
    "    \n",
    "    #Pad the sentences to the maximum length with PAD_TOKEN\n",
    "    answers_vectors_padded = zero_padding(answers_vectors)\n",
    "    \n",
    "    return answers_vectors_padded, answers_mask, answers_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [pair[0] for pair in final_qa_pairs]\n",
    "question_vectors_padded, questions_mask, questions_lengths  = questions2vectors(voc,questions)\n",
    "print()\n",
    "answers = [pair[1] for pair in final_qa_pairs]\n",
    "answers_vectors_padded, answers_mask, answers_lengths  = answers2vectors(voc,answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Input, we don't need the mask, as we will use the lengths function in pack_padded_sequence to determine in paddings\n",
    "# In Output, we need the mask function, as it will be used in the loss (maskNLLLoss)\n",
    "batch_size = 64\n",
    "dataset = torch.utils.data.TensorDataset(question_vectors_padded, questions_lengths , answers_vectors_padded, answers_mask,answers_lengths)\n",
    "train_dataloader  = torch.utils.data.DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "# max_target_len = max(answer_lengths) ---> Will be needed later in the train_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([26875, 11])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_vectors_padded.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([26875, 11])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_vectors_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "\n",
    "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
    "        #   because our input size is a word embedding with number of features == hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # Convert word indexes to embeddings\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        print(\"Embedded from Encoder:\",embedded.shape)\n",
    "        print(\"Input Lengths from Encoder:\",len(input_lengths))\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths,batch_first=True,enforce_sorted=False)\n",
    "        # Forward pass through GRU\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        # Unpack padding\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs,batch_first=True)\n",
    "        # Sum bidirectional GRU outputs\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        # Return output and final hidden state\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luong attention layer\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super( ).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Calculate the attention weights (energies) based on the given method\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        attn_energies = attn_energies.t()\n",
    "\n",
    "        # Return the softmax normalized probability scores (with added dimension)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super( ).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step (word) at a time\n",
    "        # Get embedding of current input word\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # Forward through unidirectional GRU\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        # Calculate attention weights from the current GRU output\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        # Predict next word using Luong eq. 6\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        # Return output and final hidden state\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[0.0575, 0.2809, 0.7025, 0.7735, 0.8638],\n",
       "        [0.8009, 0.4077, 0.0554, 0.4601, 0.7082],\n",
       "        [0.5057, 0.2061, 0.4572, 0.8015, 0.1893],\n",
       "        [0.4699, 0.8882, 0.5105, 0.9807, 0.7253],\n",
       "        [0.0139, 0.4963, 0.5086, 0.6076, 0.6502],\n",
       "        [0.8538, 0.6370, 0.7128, 0.4015, 0.3457],\n",
       "        [0.8452, 0.1311, 0.8235, 0.7654, 0.5286],\n",
       "        [0.6603, 0.7052, 0.1643, 0.3576, 0.6263],\n",
       "        [0.1488, 0.6658, 0.7640, 0.9306, 0.4674],\n",
       "        [0.6604, 0.7372, 0.0576, 0.0771, 0.9560],\n",
       "        [0.8724, 0.7236, 0.6678, 0.2173, 0.3163],\n",
       "        [0.3437, 0.7498, 0.5143, 0.7214, 0.6581],\n",
       "        [0.1668, 0.4992, 0.1329, 0.9904, 0.5818],\n",
       "        [0.8351, 0.9924, 0.5670, 0.1276, 0.7069],\n",
       "        [0.5289, 0.8241, 0.5616, 0.8331, 0.3413],\n",
       "        [0.4890, 0.2811, 0.3797, 0.7544, 0.2458],\n",
       "        [0.5535, 0.2952, 0.6204, 0.6572, 0.6800],\n",
       "        [0.1511, 0.8952, 0.6000, 0.2579, 0.9322],\n",
       "        [0.9165, 0.1233, 0.5141, 0.3705, 0.1709],\n",
       "        [0.4647, 0.7057, 0.7539, 0.4725, 0.2296],\n",
       "        [0.3271, 0.2591, 0.4008, 0.3346, 0.8226],\n",
       "        [0.9520, 0.0695, 0.5325, 0.5725, 0.7699],\n",
       "        [0.2290, 0.7139, 0.7496, 0.5763, 0.1199],\n",
       "        [0.9991, 0.3116, 0.6081, 0.6411, 0.1984],\n",
       "        [0.7686, 0.3861, 0.6076, 0.3376, 0.4944],\n",
       "        [0.3630, 0.7899, 0.9520, 0.3800, 0.7141],\n",
       "        [0.7707, 0.5719, 0.0134, 0.8978, 0.4844],\n",
       "        [0.6846, 0.8211, 0.0712, 0.3670, 0.4711],\n",
       "        [0.1223, 0.2815, 0.2036, 0.6335, 0.2194],\n",
       "        [0.9824, 0.0625, 0.7624, 0.0116, 0.4492],\n",
       "        [0.7792, 0.8533, 0.0849, 0.1047, 0.0274],\n",
       "        [0.2429, 0.5725, 0.4437, 0.0091, 0.1299],\n",
       "        [0.6972, 0.0240, 0.4490, 0.6392, 0.6469],\n",
       "        [0.9951, 0.4536, 0.9851, 0.2151, 0.8123],\n",
       "        [0.1986, 0.0139, 0.1809, 0.7024, 0.2990],\n",
       "        [0.5027, 0.2981, 0.7481, 0.8080, 0.8099]]), batch_sizes=tensor([6, 5, 4, 4, 4, 3, 3, 2, 2, 2, 1]), sorted_indices=tensor([2, 3, 5, 4, 1, 0]), unsorted_indices=tensor([5, 4, 0, 1, 3, 2]))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand([6,11,5])\n",
    "l = [1,2,11,10,5,7]\n",
    "nn.utils.rnn.pack_padded_sequence(x, l,batch_first=True,enforce_sorted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss#.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
    "          encoder_optimizer, decoder_optimizer, batch_size, clip):\n",
    "\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Set device options\n",
    "    input_variable = input_variable#.to(device)\n",
    "    target_variable = target_variable#.to(device)\n",
    "    mask = mask#.to(device)\n",
    "    # Lengths for rnn packing should always be on the cpu\n",
    "    lengths = lengths.tolist()\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_TOKEN for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input#.to(device)\n",
    "\n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "    # Determine if we are using teacher forcing this iteration\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Teacher forcing: next input is current target\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # No teacher forcing: next input is decoder's own current output\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input#.to(device)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    # Perform backpropatation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(model_name, voc, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, \n",
    "               decoder_n_layers, n_iteration, batch_size, print_every, save_every, clip):\n",
    "\n",
    "    # Load batches for each iteration\n",
    "    #training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])for _ in range(n_iteration)]\n",
    "                      \n",
    "\n",
    "    # Initializations\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "   \n",
    "\n",
    "    # Training loop\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        \n",
    "        # Extract fields from batch\n",
    "        # input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "        input_variable_padded, input_lengths , target_vectors_padded, target_mask,target_lengths =  next(iter(train_dataloader))  \n",
    "        max_target_len = max(target_lengths)\n",
    "        # Run a training iteration with batch\n",
    "        loss = train(input_variable_padded, input_lengths, target_vectors_padded, target_mask, max_target_len, encoder,\n",
    "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        print_loss += loss\n",
    "\n",
    "        # Print progress\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_variable_padded, input_lengths , target_vectors_padded, target_mask,target_lengths =  next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 11]), torch.Size([64]))"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_variable_padded.shape,input_lengths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "#attn_model = 'general'\n",
    "#attn_model = 'concat'\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "\n",
    "# Use appropriate device\n",
    "encoder = encoder#.to(device)\n",
    "decoder = decoder#.to(device)\n",
    "print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4976, 4973)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc.num_words, len(voc.word2count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4976"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc.index2word.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimizers ...\n",
      "Starting Training!\n",
      "Initializing ...\n",
      "Training...\n",
      "Embedded from Encoder: torch.Size([64, 11, 500])\n",
      "Input Lengths from Encoder: 64\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (11) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-2d3868de662b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Run training iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting Training!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m trainIters(model_name, voc, encoder, decoder, encoder_optimizer, decoder_optimizer,\n\u001b[0m\u001b[1;32m     32\u001b[0m            \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_n_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_n_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m            print_every, save_every, clip)\n",
      "\u001b[0;32m<ipython-input-156-234d732527ef>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(model_name, voc, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, n_iteration, batch_size, print_every, save_every, clip)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mmax_target_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Run a training iteration with batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         loss = train(input_variable_padded, input_lengths, target_vectors_padded, target_mask, max_target_len, encoder,\n\u001b[0m\u001b[1;32m     24\u001b[0m                      decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-155-4c79f53fb5a8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_teacher_forcing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_target_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             decoder_output, decoder_hidden = decoder(\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-141-63942209252b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_step, last_hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mrnn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Calculate attention weights from the current GRU output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-140-f9085436d190>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mattn_energies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'dot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mattn_energies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Transpose max_length and batch_size dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-140-f9085436d190>\u001b[0m in \u001b[0;36mdot_score\u001b[0;34m(self, hidden, encoder_output)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdot_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgeneral_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (11) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 4000\n",
    "print_every = 1\n",
    "save_every = 500\n",
    "\n",
    "# Ensure dropout layers are in train mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Initialize optimizers\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    " \n",
    "# If you have cuda, configure cuda to call\n",
    "for state in encoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "for state in decoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "# Run training iterations\n",
    "print(\"Starting Training!\")\n",
    "trainIters(model_name, voc, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, n_iteration, batch_size,\n",
    "           print_every, save_every, clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

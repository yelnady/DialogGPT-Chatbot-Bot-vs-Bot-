{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from transformers import  AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# tokenizer  = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# tokenizer.save_pretrained('saved_model')\n",
    "# model.save_pretrained('saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"saved_model\")\n",
    "tokenizer =  AutoTokenizer.from_pretrained(\"saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cocalc": {
     "outputs": {
      "0": {
       "name": "input",
       "opts": {
        "password": false,
        "prompt": ">> Prompt:"
       },
       "output_type": "stream"
      }
     }
    },
    "collapsed": false,
    "jupyter": {
    },
    "scrolled": true,
    "tags": [
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": ">> Prompt: "
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-59a0b0888d20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn_questions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">> Prompt:\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_questions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uni/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m             )\n\u001b[0;32m--> 848\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    849\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uni/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# #bot2bot With History Saved\n",
    "# n_questions = 10\n",
    "\n",
    "# prompt = tokenizer.encode(input(\">> Prompt:\") + tokenizer.eos_token, return_tensors='pt' )\n",
    "\n",
    "# for question in range(n_questions):\n",
    "\n",
    "#     if question==0:\n",
    "#         bot1_input = prompt\n",
    "#     else:\n",
    "#         bot1_input = history\n",
    "\n",
    "#     history =  model.generate(bot1_input, max_length=10_000, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "#     # We just need to print the last output from the history, which is similar to the length of the bot_input we fed\n",
    "#     bot1_result = tokenizer.decode(history[:, bot1_input.shape[-1]:][0], skip_special_tokens=True)\n",
    "#     print(\"DialoGPT1: {}\".format(bot1_result))\n",
    "     \n",
    "#     bot2_input = history\n",
    "#     history =  model.generate(bot2_input, max_length=10_000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "#     bot2_result = tokenizer.decode(history[:, bot2_input.shape[-1]:][0], skip_special_tokens=True)\n",
    "#     print(\"DialoGPT2: {}\".format(bot2_result))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "set_of_questions=[ 'Can we go out together?', 'What is your color?', 'Are you a robot?', 'Tell me something', 'How can you help me?', 'What can you do?',\n",
    "                 'Happy birthday!', 'Do you enjoy any music?', 'Do you have any favorite games?' , 'What do you think about yourself?', 'I enjoy hiking and the outdoors.','Can you make it to dinner tonight?', 'What is your best movie?',\"What's one thing that can instantly make your day better?\",\"In the summer, would you rather go to the beach or go camping?\",\"Do you have any pet peeves?\",\"Would you rather cook or order in?\",\"What's your favorite board game?\",\"Is there any product that you couldn't live without?\",\"What type of role do you want to take on after this one?\",\"What do you remember most about your first job?\",\n",
    "                 \"What’s the worst job you’ve ever had?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "cocalc": {
     "outputs": {
      "0": {
       "name": "input",
       "opts": {
        "password": false,
        "prompt": ">> Prompt:"
       },
       "output_type": "stream",
       "value": "what is your favorite color?"
      }
     }
    },
    "collapsed": false,
    "tags": [
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT1: Do you have any favorite games?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT2: I don't really have a favorite, but I do like the ones that I play.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT1: I like the ones I play too.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT2: What's one thing that can instantly make your day better?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT1: I'm not sure, but I'm going to say a good question.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT2: I'm going to say a good question.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT1: I'm going to say a good answer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT2: Is there any product that you couldn't live without?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT1: I don't know, I'm not a fan of the stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT2: I'm not a fan of the stuff either.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT1: What's your favorite board game?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT2: I'm not sure, I've never played a board game.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT1: What is your color?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT2: I'm a red head.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT1: Do you enjoy any music?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT2: I enjoy music, but I don't really listen to it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT1: I'm the same way. I don't really listen to music, but I don't really listen to music.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT2: I listen to music, but I don't listen to music.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT1: Tell me something\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT2: I'm a girl.\n"
     ]
    }
   ],
   "source": [
    "#bot2bot Without History\n",
    "n_questions = 10\n",
    "\n",
    "# prompt = tokenizer.encode(+ tokenizer.eos_token, return_tensors='pt' ) \n",
    "bot2_input =''\n",
    "bot2_result = ''\n",
    "for question in range(n_questions):\n",
    "    if question==0:\n",
    "        bot1_result = random.choice(set_of_questions) \n",
    "    else:\n",
    "        bot1_input = history\n",
    "        history =  model.generate(bot1_input, max_length=100_000, pad_token_id=tokenizer.eos_token_id)[:, bot1_input.shape[-1]:]\n",
    "        bot1_result = tokenizer.decode(history[0], skip_special_tokens=True)\n",
    "    # We just need to print the last output from the history, which is similar to the length of the bot_input we fed\n",
    "    \n",
    "    if bot1_result == bot2_result:\n",
    "        bot1_result = random.choice(set_of_questions)\n",
    "        history = tokenizer.encode(bot1_result + tokenizer.eos_token, return_tensors='pt' )\n",
    "    \n",
    "    print(\"DialoGPT1: {}\".format(bot1_result))\n",
    "    \n",
    "    if question==0:\n",
    "        bot2_input = tokenizer.encode(bot1_result + tokenizer.eos_token, return_tensors='pt' )\n",
    "    else:\n",
    "        bot2_input = history\n",
    "        \n",
    "    history =  model.generate(bot2_input, max_length=100_000, pad_token_id=tokenizer.eos_token_id)[:, bot2_input.shape[-1]:]\n",
    "    bot2_result = tokenizer.decode(history[0], skip_special_tokens=True)\n",
    "\n",
    "    if bot1_result == bot2_result:\n",
    "        bot2_result = random.choice(set_of_questions)\n",
    "        history = tokenizer.encode(bot2_result + tokenizer.eos_token, return_tensors='pt' )\n",
    "    print(\"DialoGPT2: {}\".format(bot2_result))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "chat_history_bot2bot = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#bot2bot Without History\n",
    "n_questions = 10\n",
    "\n",
    "\n",
    "bot2_input =''\n",
    "bot2_result = ''\n",
    "for question in range(n_questions):\n",
    "    if question==0:\n",
    "        bot1_input = tokenizer.encode(random.choice(set_of_questions) + tokenizer.eos_token, return_tensors='pt' ) \n",
    "    else:\n",
    "        bot1_input = history\n",
    "\n",
    "    history =  model.generate(bot1_input, max_length=10_000, pad_token_id=tokenizer.eos_token_id)[:, bot1_input.shape[-1]:]\n",
    "    # We just need to print the last output from the history, which is similar to the length of the bot_input we fed\n",
    "    bot1_result = tokenizer.decode(history[0], skip_special_tokens=True)\n",
    "    if bot1_result == bot2_result:\n",
    "        bot1_result = random.choice(set_of_questions)\n",
    "        history = tokenizer.encode(bot1_result + tokenizer.eos_token, return_tensors='pt' )\n",
    "    \n",
    "    print(\"DialoGPT1: {}\".format(bot1_result))\n",
    "     \n",
    "    bot2_input = history\n",
    "    history =  model.generate(bot2_input, max_length=10_000, pad_token_id=tokenizer.eos_token_id)[:, bot2_input.shape[-1]:]\n",
    "    bot2_result = tokenizer.decode(history[0], skip_special_tokens=True)\n",
    "\n",
    "    if bot1_result == bot2_result:\n",
    "        bot2_result = random.choice(set_of_questions)\n",
    "        history = tokenizer.encode(bot2_result + tokenizer.eos_token, return_tensors='pt' )\n",
    "    print(\"DialoGPT2: {}\".format(bot2_result))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "cocalc": {
     "outputs": {
      "0": {
       "name": "input",
       "opts": {
        "password": false,
        "prompt": ">> User:"
       },
       "output_type": "stream",
       "value": "What is your biggest irrational fear?"
      },
      "2": {
       "name": "input",
       "opts": {
        "password": false,
        "prompt": ">> User:"
       },
       "output_type": "stream"
      },
      "4": {
       "name": "input",
       "opts": {
        "password": false,
        "prompt": ">> User:"
       },
       "output_type": "stream",
       "value": "Can we chat a lot?"
      },
      "6": {
       "name": "input",
       "opts": {
        "password": false,
        "prompt": ">> User:"
       },
       "output_type": "stream"
      }
     }
    },
    "collapsed": false,
    "tags": [
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": ">> User: What is your biggest irrational fear?"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I'm afraid of heights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": ">> User: "
    }
   ],
   "source": [
    "#Person to bot\n",
    "\n",
    "n_questions = 100\n",
    "for question in range(n_questions):\n",
    "\n",
    "    new_question = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token\n",
    "                                  , return_tensors='pt' )\n",
    "    \n",
    "    if question==0:\n",
    "        bot_input = new_question\n",
    "    else:\n",
    "        bot_input = torch.cat([chat_history, new_question], dim=-1)\n",
    "    chat_history = model.generate(bot_input, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history[:, bot_input.shape[-1]:][0], skip_special_tokens=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def get_response_bot2human(question):\n",
    "    new_question = tokenizer.encode(question + tokenizer.eos_token, return_tensors='pt' )\n",
    "     if chat_history_bot2human is None:\n",
    "        bot_input = new_question\n",
    "    else:\n",
    "        bot_input = torch.cat([chat_history_bot2human, new_question], dim=-1)\n",
    "    \n",
    "    chat_history_bot2human = model_bot2human.generate(bot_input, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    return tokenizer.decode(chat_history_bot2human[:, bot_input.shape[-1]:][0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "duffy",
   "resource_dir": "/projects/3e080ba6-1476-4b72-953b-1b591cbf600a/.local/share/jupyter/kernels/duffy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}